

\section{Project Information}
\subsection{Scheme Description}
Sparse matrices are extensively used in scientific computing, however there is no automatic differentiation package in Julia yet to handle sparse matrix operations. This project utilizes the reversible embedded domain-specific language NiLang.jl to differentiate sparse matrix operations by writing the sparse matrix operations in a reversible style. The generated backward rules are ported to ChainRules.jl as an extension, so that one can access these features in an automatic differentiation package like Zygote, Flux and Diffractor directly.
\subsection{Time Planning}

This project is shipped by four (mostly) sequential stages:
\begin{enumerate}[(1)]
    \item Implement low level operations by NiLang.
    \item Carefully test by CI and export chain rules into ChainRules.jl. 
    \item Implement high level operations and perform AD operations.
    \item Add some use cases and enhance docs. 
\end{enumerate}

I list the timeline of this project in the form of Gantt chart.
\begin{enumerate}[(1)]
    \item 1st Jul - 31st Jul \quad Implement low level operations by NiLang. 
    \item 1st Aug - 15th Aug   \quad Carefully test by CI and export chain rules into ChainRules.jl.
    \item 15th Sep - 20th Sep \quad  Implement high level operations and perform AD operations. 
    \item 21st Sep - 30th Sep \quad Add some use cases and enhance docs.
\end{enumerate}
\vspace{0.5cm}

\input{gantt.tex}
\vspace{0.5cm}  

\section{Project Summary}
\subsection{Project Output}
\begin{enumerate}[(1)]
    \item Differentiate sparse matrix operations in Julia base by rewriting the sparse functions in NiLang.jl.  
    \textbf{Status: } \textit{Already completed. Black-box tests on NiLang programs have passed in CI.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/20}{Related PR}
    \item Port the generated backward rules to ChainRules.jl as an extension. \\ 
    \textbf{Status: } \textit{Already completed. Backward rules have been checked by ChainRulesTestUtils.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/39}{Related PR}
    \item Release an open source julia package with test coverage over 85\%.  \\
    \textbf{Status: } \textit{Test coverage achieved at 87\%.} 
    \href{https://app.codecov.io/gh/jieli-matrix/SparseArraysAD.jl}{Code Converage Report} 
    \item Add some use cases for getting a start.  \\
    \textbf{Status: } \textit{Already completed. A simple use case is shown in ReadMe.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/34}{Related PR}
\end{enumerate}
\subsection{Scheme Progress}
\begin{enumerate}
    \item 1st Jul - 31st Jul \quad Implement low level operations by NiLang.  \\
    \textit{I have rewritten almost all sparse matrix multiplication and dot operations in Julia base by NiLang.  
    I list the implemented sparse matrix operators as follows. 
    All the functions listed have passed CI tests and the code coverage have achieved over 85\%.}
    \begin{lstlisting}[language=Julia]
        function imul!(C::StridedVecOrMat, A::AbstractSparseMatrix{T}, B::DenseInputVecOrMat, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, xA::Adjoint{T, <:AbstractSparseMatrix}, B::DenseInputVecOrMat, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, X::DenseMatrixUnion, A::AbstractSparseMatrix{T}, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, X::Adjoint{T1, <:DenseMatrixUnion}, A::AbstractSparseMatrix{T2}, α::Number, β::Number) where {T1, T2}
        function imul!(C::StridedVecOrMat, X::DenseMatrixUnion, xA::Adjoint{T, <:AbstractSparseMatrix}, α::Number, β::Number) where T
        function idot(r, A::SparseMatrixCSC{T},B::SparseMatrixCSC{T}) where {T}
        function idot(r, x::AbstractVector, A::AbstractSparseMatrix{T1}, y::AbstractVector{T2}) where {T1, T2}
        function idot(r, x::SparseVector, A::AbstractSparseMatrix{T1}, y::SparseVector{T2}) where {T1, T2}
    \end{lstlisting}  
    \item 1st Aug - 15th Aug   \quad Carefully test by CI and export chain rules into ChainRules.jl.  \\
    \textit{I have exported the backward rules into ChainRules.jl. I used ChainRulesTestUtils.jl to test correctness and robustness of 
    sparse matrix AD rules. All the rules have passed CI tests.}
    \item 15th Sep - 20th Sep \quad  Implement high level operations and perform AD operation. \\
    \textit{I have implemented low rank svd algorithm in Julia. To perform AD on low\_rank\_svd algorithm, I wrapped backward rules of 
    QR decomposition since QR rules haven't been ensembled in ChainRules. I used finite differences method to check the correctness 
    of the algorithm. All the code have passed CI tests and the code converage have achieved at 87\% finally.}
    \item 21st Sep - 30th Sep \quad Add some use cases and enhance docs. \\
    \textit{A simple use case has been added into ReadMe in the project.}
\end{enumerate}
    
\subsection{Problems and Solutions}  
To complete the projcet, one should be equipped with both good sense of numerical linear algebra and automatic differentiation \textbf{in 
theory} and familiarity with NiLang.jl, ChainRules.jl and so on \textbf{in practice}. It took me much of time to figure out how automatic 
differentiation works out in computation process. In this section, I would only introduce the problems and solutions I came cross in practice.
\subsubsection{Incorrect gradient of sparse vector}
When I tried to export chain rules of function \textit{dot}, all the checks failed in ChainRulesTestUtils.
Then, I printed the gradient calculated by NiLang and found it proved to be zero which was really abnormal. 
I fed back the strange phenomenon to mentor and he told me that iterator can not carry gradients and the dangerous feature would 
be removed from NiLang in the future. After I corrected the codes according to his suggestions, all the checks passed.
The problem also gave a good reason to remove iterator support to avoid potential missing of gradients in NiLang, which had been metioned 
in \href{https://github.com/GiggleLiu/NiLang.jl/issues/76}{this issue}.  
\subsubsection{Test on Generated Rules}
In our project, I calculated the gradient of sparse matrix in reversal mode in NiLang. To test the correctness 
of the gradient, I should calculate gradient in an another way. There are many choices such as FiniteDifferences, ForwardDiff and 
so on. After discussing with mentors, I decided to use ChainRulesTestUtils to check the correctness of the gradient, which was highly recommended by Lyndon White in JuliaCon2020. At first, I implemented tangent of 
sparse matrix by myself, it is a bit hard task for me since there are no related examples. Later, mentor and I talked about this problem with Lyndon White, the 
core founder of ChainRules, in slack and he mentioned rand\_tangent method may help. I read related source code in CRTU and made some changes so as to keep the 
adjoint gradient could keep the sparsity. Finally all the rules checked by CRTU and passed all the tests successfully. 
\subsubsection{Add support for QR rules}
The low\_rank\_svd involved with QR decomposition which was used to find the approximate basis. 
However, backward rules of QR decomposition were ensembled neither in ChainRules nor Zygote. In order to 
calculate the gradient of low\_rank\_svd, I wrapped the QR rules in ChainRules and checked by CRTU in help of mentor. 
I would like to enhance the implementation of QR rules (keep Q factor and R factor in compactWY format) and pull request to 
ChainRules.jl in the future. 
\subsection{Development Quality}
You could give a self evaluation of development quality
\subsection{Communication and Feedback with Mentor}

