

\section{Project Information}
\subsection{Scheme Description}
Sparse matrices are extensively used in scientific computing, however there is no automatic differentiation package in Julia yet to handle sparse matrix operations. This project utilizes the reversible embedded domain-specific language NiLang.jl to differentiate sparse matrix operations by writing the sparse matrix operations in a reversible style. The generated backward rules are ported to ChainRules.jl as an extension, so that one can access these features in an automatic differentiation package like Zygote, Flux and Diffractor directly.
\subsection{Time Planning}

This project is shipped by four (mostly) sequential stages:
\begin{enumerate}[(1)]
    \item Implement low level operations by NiLang.
    \item Carefully test by CI and export chain rules into ChainRules.jl. 
    \item Implement high level operations and perform AD operations.
    \item Add some use cases and enhance docs. 
\end{enumerate}

I list the timeline of this project in the form of Gantt chart.
\begin{enumerate}[(1)]
    \item 1st Jul - 31st Jul \quad Implement low level operations by NiLang. 
    \item 1st Aug - 15th Aug   \quad Carefully test by CI and export chain rules into ChainRules.jl.
    \item 15th Sep - 20th Sep \quad  Implement high level operations and perform AD operations. 
    \item 21st Sep - 30th Sep \quad Add some use cases and enhance docs.
\end{enumerate}
\vspace{0.5cm}

\input{gantt.tex}
\vspace{0.5cm}  

\section{Project Summary}
\subsection{Project Output}
\begin{enumerate}[(1)]
    \item Differentiate sparse matrix operations in Julia base by rewriting the sparse functions in NiLang.jl.  
    \textbf{Status: } \textit{Already completed. Black-box tests on NiLang programs have passed in CI.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/20}{Related PR}
    \item Port the generated backward rules to ChainRules.jl as an extension. \\ 
    \textbf{Status: } \textit{Already completed. Backward rules have been checked by ChainRulesTestUtils.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/39}{Related PR}
    \item Release an open source julia package with test coverage over 85\%.  \\
    \textbf{Status: } \textit{Test coverage achieved at 87\%.} 
    \href{https://app.codecov.io/gh/jieli-matrix/SparseArraysAD.jl}{Code Converage Report} 
    \item Add some use cases for getting a start.  \\
    \textbf{Status: } \textit{Already completed. A simple use case is shown in ReadMe.}  
    \href{https://github.com/jieli-matrix/SparseArraysAD.jl/pull/34}{Related PR}
\end{enumerate}
\subsubsection{Demo}
In this section, an eigen-solver case and a low rank SVD case are illustrated to show how to use the package. 
\paragraph{Eigen-solver case} Here, I want to calculate the eigenvector $\vec{x}$ corresponding to 
the maximum eigenvalue of matrix $\mathbf{A}$. 
\begin{lstlisting}[language=Julia]
    # find max eigenvalue by power method
function power_max_eigen(A, x, target; niter=100)
    for i=1:niter
        x = A * x
        x /= norm(x)
    end
    return abs(x' * target)
end
\end{lstlisting}

I use the similarity between $\vec{x}$ and the target vector as loss function, which would adjust $\vec{x}$
to the expected one. Since I have exported rules on sparse matrix into ChainRules, I could 
calculate gradient of power\_max\_eigen simply as follows.  

\begin{lstlisting}[language=Julia]
    using Zygote
    using SparseArrays, LinearAlgebra
    using BenchmarkTools
    # generate test data
    A = sprand(5000, 5000, 0.1)
    x = randn(5000)
    target = randn(5000)

    # Zygote is able to generate correct gradient for sparse matrix and dense vectors
    # in computation form of dense arrays  
    @btime max_ga_z, max_gx_z, max_gt_z = Zygote.gradient(power_max_eigen, $A, $x, $target) #12.954 s (4272 allocations: 27.91 GiB)
    using SparseArraysAD
    # The gradient generated by SparseArraysAD is much faster
    # since it keeps the orginal type in computation process 
    @btime max_ga, max_gx, max_gt = Zygote.gradient(power_max_eigen, $A, $x, $target) # 6.180 s (5072 allocations: 16.75 GiB)
\end{lstlisting}

Here I compared the memory allocations and computation speed between our package and Zygote.
You will see that using SparseArraysAD would not only speed up the computation process 
but also save much memory since our implementation does not convert a sparse matrix 
to a dense arrays in gradient computation.

\paragraph{Low Rank SVD case}
One of the most important steps in low\_rank\_algorithm is to calculate the approximate basis, which 
invloves with matrix multiplication and QR decomposition. I ensembled QR rules in the package since 
official QR rules has not been included in ChainRules yet. SparseArraysAD could speed up the solution 
since I have exported rules of sparse matrix multiplication.

\begin{lstlisting}[language=Julia]
function svd_loss(A::AbstractSparseMatrix{T}, z::Vector{T}, r::Int) where T
    U, S, Vt = low_rank_svd(A, r)
    residual_mat = U * Diagonal(S) * Vt - U * Diagonal(z) * Vt
    return tr(residual_mat'*residual_mat) # square of Frobenius norm
end

# generate test data
r = 10
A = sprand(100, r, 0.2) * Diagonal(rand(r)) * sprand(r, 50, 0.2)
z = rand(r)

grad_A = Zygote.gradient(A -> svd_loss(A, z, r), A)[1] 
grad_fA = FiniteDifferences.grad(FiniteDifferences.central_fdm(5, 1), A -> svd_loss(A, z, r), A)[1]
\end{lstlisting}

I use FiniteDifferences to check the results computed by SparseArraysAD and all checks 
passed. 

\subsection{Scheme Progress}
\begin{enumerate}
    \item 1st Jul - 31st Jul \quad Implement low level operations by NiLang.  \\
    \textit{I have rewritten almost all sparse matrix multiplication and dot operations in Julia base by NiLang.  
    I list the implemented sparse matrix operators as follows. 
    All the functions listed have passed CI tests and the code coverage have achieved over 85\%.}
    \begin{lstlisting}[language=Julia]
        function imul!(C::StridedVecOrMat, A::AbstractSparseMatrix{T}, B::DenseInputVecOrMat, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, xA::Adjoint{T, <:AbstractSparseMatrix}, B::DenseInputVecOrMat, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, X::DenseMatrixUnion, A::AbstractSparseMatrix{T}, α::Number, β::Number) where T
        function imul!(C::StridedVecOrMat, X::Adjoint{T1, <:DenseMatrixUnion}, A::AbstractSparseMatrix{T2}, α::Number, β::Number) where {T1, T2}
        function imul!(C::StridedVecOrMat, X::DenseMatrixUnion, xA::Adjoint{T, <:AbstractSparseMatrix}, α::Number, β::Number) where T
        function idot(r, A::SparseMatrixCSC{T},B::SparseMatrixCSC{T}) where {T}
        function idot(r, x::AbstractVector, A::AbstractSparseMatrix{T1}, y::AbstractVector{T2}) where {T1, T2}
        function idot(r, x::SparseVector, A::AbstractSparseMatrix{T1}, y::SparseVector{T2}) where {T1, T2}
    \end{lstlisting}  
    \item 1st Aug - 15th Aug   \quad Carefully test by CI and export chain rules into ChainRules.jl.  \\
    \textit{I have exported the backward rules into ChainRules.jl. I used ChainRulesTestUtils.jl to test correctness and robustness of 
    sparse matrix AD rules. All the rules have passed CI tests.}
    \item 15th Sep - 20th Sep \quad  Implement high level operations and perform AD operation. \\
    \textit{I have implemented low rank svd algorithm in Julia. To perform AD on low\_rank\_svd algorithm, I wrapped backward rules of 
    QR decomposition since QR rules haven't been ensembled in ChainRules. I used finite differences method to check the correctness 
    of the algorithm. All the code have passed CI tests and the code converage have achieved at 87\% finally.}
    \item 21st Sep - 30th Sep \quad Add some use cases and enhance docs. \\
    \textit{A simple use case has been added into ReadMe in the project.}
\end{enumerate}
    
\subsection{Problems and Solutions}  
To complete the projcet, one should be equipped with both good sense of numerical linear algebra and automatic differentiation \textbf{in 
theory} and familiarity with NiLang.jl, ChainRules.jl and so on \textbf{in practice}. It took me much of time to figure out how automatic 
differentiation works out in computation process. In this section, I would only introduce the problems and solutions I came cross in practice.
\subsubsection{Incorrect gradient of sparse vector}
When I tried to export chain rules of function \textit{dot}, all the checks failed in ChainRulesTestUtils.
Then, I printed the gradient calculated by NiLang and found it proved to be zero which was really abnormal. 
I fed back the strange phenomenon to mentor and he told me that iterator can not carry gradients and the dangerous feature would 
be removed from NiLang in the future. After I corrected the codes according to his suggestions, all the checks passed.
The problem also gave a good reason to remove iterator support to avoid potential missing of gradients in NiLang, which had been metioned 
in \href{https://github.com/GiggleLiu/NiLang.jl/issues/76}{this issue}.  
\subsubsection{Test on Generated Rules}
In our project, I calculated the gradient of sparse matrix in reversal mode in NiLang. To test the correctness 
of the gradient, I should calculate gradient in an another way. There are many choices such as FiniteDifferences, ForwardDiff and 
so on. After discussing with mentors, I decided to use ChainRulesTestUtils to check the correctness of the gradient, which was highly recommended by Lyndon White in JuliaCon2020. At first, I implemented tangent of 
sparse matrix by myself, it is a bit hard task for me since there are no related examples. Later, mentor and I talked about this problem with Lyndon White, the 
core founder of ChainRules, in slack and he mentioned rand\_tangent method may help. I read related source code in CRTU and made some changes so as to keep the 
adjoint gradient could keep the sparsity. Finally all the rules checked by CRTU and passed all the tests successfully. 
\subsubsection{Add support for QR rules}
The low\_rank\_svd involved with QR decomposition which was used to find the approximate basis. 
However, backward rules of QR decomposition were ensembled neither in ChainRules nor Zygote. In order to 
calculate the gradient of low\_rank\_svd, I wrapped the QR rules in ChainRules and checked by CRTU in help of mentor. 
I would like to enhance the implementation of QR rules (keep Q factor and R factor in compactWY format) and pull request to 
ChainRules.jl in the future. 
\subsection{Development Quality}
\begin{enumerate}
    \item Code style  \\
    Blue style was recommended in Julia. Since I was a freshman in Julia, I
    read related code and initiated their writing style. I was really amazing at 
    the delicate code structure when I read source code in SparseArrays, FiniteDifferences 
    and ChainRulesTestUtils. I tried to initiate such projects and made code in the project more 
    readable and clearer.   
    \item Project Structure \\
    I used the PkgTemplate in Julia, which was the standard project template in Julia. 
    Core source code and auxiliary test code were splited in different folders. It was easy to 
    perform unit tests and add or remove dependencies for developers. Besides, it was also 
    convenient for users to add the package and use it. 
    \item Robustness \\
    I wrote test code for each function in source code and code coverage achieved at 87\% finally. Furthermore, Continuous Integration (CI)
    has been set in Github Actions, which would be triggered automatically when I commited code to 
    github repository. Updated changes would be merged into master branch only when all the checks 
    passed in CI.   
    \item Work Pipeline \\
    I really appreciated great efforts of mentors. When I came across abnormal bugs, they would discuss the problems with me 
    and provide useful suggestions. After the code passed CI tests, they would review my code carefully and then approved or merged the 
    updates into master branch. 
\end{enumerate}



\subsection{Communication and Feedback with Mentor}
Community mentors Jinguo and Jiuning kept track of the project and guided me patiently during the 
whole process. Routine meetings were held to talk development plan and we debug the broken programs by 
living coding and discussion on github. They gave insightful feedback to me which would help me to improve 
myself in the future listed as follows. 

\begin{itemize}
    \item Learn how to write a good issue: declare the problems clearly and don't let others 
    to guess your problems from logs. 
    \item Communicate with other developers in the community and don't hesitate to ask questions. 
    \item Keep an open mind to other AD packages and use even improve them. 
\end{itemize}
